#语音输入：一种新的触摸屏设备听写语音交互模型

**摘要    **使用语音识别的口述可以作为触摸屏设备的有效输入方法。然而，今天的听写系统会遵循一种精神破坏性的语音交互模式：用户必须首先制定话语，然后制作话语，就像用录音机一样。由于话语在用户说完之前不会被转录，因此整个输出都会出现，用户必须打破思路才能验证并纠正。在本文中，我们介绍语音打字，一种新的语音交互模式，用户的话语在他们产生时被转录，以实现实时错误识别。为了快速校正，用户使用触摸手势来利用标记菜单。语音输入旨在为您创建类似于秘书类型的体验，同时您可以监控并更正文本。在一项用户研究中，参与者使用语音打字和传统的听写方式撰写电子邮件，他们不仅报告了对语音打字的认知需求较低，而且还显示出相对减少了29％的用户更正。总的来说，他们还喜欢语音输入。
**关键词    **语音识别; 听写;多式联运; 纠错; 语音用户界面

##简介

触摸屏设备（如智能手机，平板电脑和桌面）通常使用软键盘进行输入。然而，由于缺乏触觉反馈[^12]和其他人体工程学问题，如“胖手指问题”[^10]，打字可能具有挑战性。使用语音识别的自动听写可以作为自然和有效的输入模式，提供几个潜在的优点。首先，据报告，语音吞吐量至少比硬件QWERTY键盘上的打字速度快3倍[^3]。其次，与手写或打字等其他文字输入方法相比，语音在屏幕尺寸方面具有最大的灵活性。最后，随着触摸屏设备在全世界范围内的普及，到目前为止，语音输入被广泛认为是8亿左右非识字人群唯一合理的方式[^26]。

然而，实现听写的潜力关键取决于具有合理的语音识别性能和用于快速纠正错误的直观用户界面。就性能而言，如果用户需要编辑每三个单词中的一个，这大概是与说话者无关（即不适应）的自发对话的声称的错误率（WER），无论编辑体验多么轻松可能是，用户会很快放弃其他模式的发言。幸运的是，利用MLLR和MAP声学自适应等个性化技术[^8,15]以及语言模型自适应[^5]，WER可降至低于10％的水平，这至少可用。请注意，所有商业发布的听写产品都会推荐并执行声学调整，有时甚至无需用户知道（例如，在Microsoft Windows 7 OS上听写）。

关于快速纠正错误，大多数听写系统的编辑经验值得期待。语音交互模型遵循语音记录隐喻，用户必须首先用话语形式表达他们想表达的内容，然后制作它们，就像使用录音机一样。这些话语在用户讲完之后才会被转录（如暂停或通过即按即说），此时整个输出在几秒钟的延迟后立即出现。这是为了使识别器可以有尽可能多的上下文来改善解码。换句话说，输出的实时显示会牺牲准确性。然后用户必须打破思路，逐字核实输出并纠正错误。这个过程可能会在心理上造成破坏性，耗时且令人沮丧。事实上，用户通常只花费25-30％的时间实际口授。剩下的时间用于识别和编辑转录错误[^13,18]。

在本文中，我们介绍语音打字，一种新的语音交互模式，用户的话语在他们产生时被转录，以实现实时错误识别。为了快速修正，用户利用基于手势的标记菜单，该菜单提供了多种编辑文本的方式。语音输入允许用户通过促进立即修正实时输出来影响解码。语音键入的隐喻是秘书为您打字，当您监控并使用触摸屏快速编辑文本时。

本文的重点是在言语作为主要输入形式时改进语音交互模型。我们提出两个贡献。首先，我们详细介绍语音输入，描述它是如何工作的，以及它的交互模型设计的动机。其次，我们描述了一项用户研究的结果，该研究评估了与电子邮件撰写任务中的传统听写相比，语音打字的效果。我们会报告定量和定性措施，并讨论可以对语音键入进行哪些更改以进一步增强用户体验并改善性能。我们的研究结果表明，通过防止解码错误通过即时用户反馈传播，Voice Typing可以实现比传统听写更低的用户纠错错误率。

##语音输入

如前所述，语音输入的语音交互模式遵循秘书为您输入的隐喻，同时您可以监控和纠正文本。实时监控很重要，因为它可以调节用户产生话语的速度。正如你秘书在你的话语上落后时你不会继续说话的那样，语音打字的用户自然地调整他们的发言速率以反映识别器的速度和准确度。事实上，在我们进行的探索性设计研究中，参与者通过“嘈杂的麦克风”向同盟者（即实验者）口述，我们发现，由于同盟者降低了打字速度（故意暗示对听到的内容不确定），参与者也放慢了说话速度。对于我们开发的原型，转录速度是这样的，用户以2-4个字的大块产生话语（更多细节请参见原型部分）。换句话说，对于实时反馈，在语音类型分析中，用户不会被限制在一次一个字的情况下说话，而是在离散识别中进行短暂停顿[24]，也不需要等待很长时间说完整的话语，就像传统的听写一样。相反，用户可以用与他们的思维过程相匹配的小块进行讲话。理想情况下，如果识别器能够以尽可能快的速度转录，并且用户能够以完美的准确度产生语音，则语音输入体验更像是专业速记员的口授。然而，凭借当前最先进的认识，由于错误识别需要更正，语音输入更类似于秘书或快速输入的朋友的口授。
我们现在阐明语音输入的动机以及实现其全部潜力所需的技术挑战。我们还描述了我们为触摸屏设备实施的原型。

##动机

语音输入的动机来自认知和技术考虑。从认知角度来看，人机交互研究人员早就知道，为用户操作提供实时反馈不仅有助于学习用户界面，而且还会带来更高的满意度[21]。在自然语言方面，心理语言学家已经注意到，当谈话中的讲话者交流时，听众经常以回授渠道的形式提供理解的实时反馈，例如头部点头和“呃呃”[6]。事实上，研究表明，语言处理是增量式的，即它通常每次只收录一个单词，而不是一次一个单词或句子[2,31,32]，正如理解时的眼动一样。文本生成的实时反馈也与大多数用户在键盘上输入的方式一致。一旦用户习惯了键盘布局，他们通常会监控他们的话语并实时纠正错误。通过这种方式，语音输入的交互模式已经为用户所熟悉。

从技术角度来看，语音类型化的动机是听写错误经常来自不正确的分段（尽管迄今为止我们不知道任何已发布的错误分类）。考虑一下经典的语音识别失败的例子：“很难破坏一个美丽的海滩”，因为话语“很难识别语音”。在这个例子中，由于附着，识别器已经错误地“识别”了“破坏很好”音素/ s /以“好”而不是“语音”。因为在说话时必须监视和纠正文字通常诱导人们以小块语言说话，所以用户更可能在发生分割的地方暂停。换句话说，在上面的例子中，用户更可能说出“识别 暂停 语音很难 暂停 ”，这为识别器提供了有用的分段信息。在实验部分，我们评估语音输入是否会导致更少的更正。

语音打字还有另一个潜在的技术优势。由于假定用户正在监视并纠正错误，因此可以将之前审查的文本视为用于后续识别的语言模型上下文以及用于声学的监督训练数据[8]和语言模型适应[5]。就前者而言，实时纠正可防止错误传播到后续识别。就后者而言，语音输入可以使用用户手动标记的声学和语言数据进行在线适应。没有比最终用户监督的更好的个性化培训数据。

##原型

虽然Voice Typing的技术优势非常吸引人，但实现从头开始设计的大词汇量连续语音识别（LVCSR）系统并不是一件容易的事，并且可能需要数年时间才能完全实现（请参阅相关工作部分）。在高层次上，LVCSR系统的解码通常如下进行（参见[22]进行评论）。一旦识别器检测到人类语音，它就将输入的音频处理成声音信号特征，然后将其映射到可能的声音单元（例如音素）。这些声音单元被进一步映射到可能的单词，识别器将这些单词连接成一个大格子或图形。最后，当识别器检测到话语已经结束时，它使用动态规划算法或维特比[23]找到通过网格的最佳路径。最佳路径产生最可能的单词序列（即，识别结果）。简而言之，当前的LVCSR系统在话语完成之前不会返回识别结果。如果鼓励用户出示构成完整句子的话语，则他们将不得不等待识别器在一次接收到转录文本之前检测到话语的结束。这当然是传统听写的语音交互模式。

为了支持语音类型的语音交互模型，识别器必须返回到目前为止创建的格的最佳路径。这可以通过大多数语音API暴露的识别假设来完成。不幸的是，由于超出本文范围的原因，识别假设往往质量很差。事实上，在构建原型时，我们探索利用识别假设，但由于精度低而放弃了这个想法。相反，我们决定使用LVCSR解码，但只做了一次修改。识别器检测话语结束的部分方式是寻找特定长度的静音。通常，这是默认的1-2秒。我们将此参数更改为0毫秒。结果是，每当用户暂停一秒钟，识别器会立即返回识别结果。请注意，第二个延迟是由识别器执行的其他处理引起的。

为了进一步促进实时转录的体验，我们将这种修改与两种交互设计选择相结合。首先，我们不是一次显示识别结果，而是决定逐个显示每个单词，从左到右，就像秘书刚输入文本一样。其次，了解识别器返回结果的速度并跟上用户话语的速度，我们训练用户以2-4个字的大小说话。

提供实时转录以便用户监控和识别错误仅仅是语音输入的第一个方面。其次是在触摸屏设备上快速有效地纠正错误。为了实现这个目标，我们利用了一个标记菜单，该菜单提供了多种编辑文本的方式。标记菜单允许用户通过两种方式指定一个菜单选项，可以通过调用径向菜单，或者在所需菜单项[14]的方向上做一个直线标记。在语音输入中，用户通过触摸他们想要编辑的单词来调用标记菜单。一旦他们了解了标记菜单上的可用选项，用户就可以简单地按照所需选择的方向进行手势操作。通过这种方式，标记菜单可以选择和编辑所需的单词，并为新手用户成为专家用户提供了一种途径。图1（a）显示了我们为语音输入开发的标记菜单。如果用户选择底部选项，如图1（b）所示，他们会收到所选单词的替代单词候选列表，该单词在语音社区中通常被称为n最佳列表。该列表还包含选定单词的首字母大写的选项。如果他们选择左边的选项，他们可以删除这个词。如果他们选择最佳选项，如图1（c）所示，他们可以重新说出单词或拼写字母。请注意，使用此选项，他们也可以说多个单词。最后，如果他们选择正确的选项，如图1（d）所示，他们可以为选定的单词添加标点符号。我们决定加入这个选项，因为很多用户觉得说“逗号”和“句号”这样的标点符号很麻烦和不自然。有一个单独的标点符号选项，用户不必考虑格式化，而将他们的想法集中到话语中。

![](/assets/4pic1.png)
图1.（a）语音打字标记菜单的屏幕截图，（b）选定单词的备选候选词列表，包括首字母大写的单词，（c）带有音量指示符的重复说话模式，以及（d）标点符号选项列表。

请注意，Voice Typing可以轻松利用鼠标或键盘进行更正，而不仅仅是触摸屏上的手势。然而，在本文中，我们决定着重于标记触摸屏设备的菜单，原因有两个。首先，触摸屏设备上的越来越多的应用程序现在提供听写（例如，苹果公司使用Nuance Dragon [20]，Android Speech-to-Text [28]，Windows Phone SMS听写[19]，Vlingo Virtual Assistant [33]等）。其次，触摸屏设备提供了一种独特的机会来利用基于触摸的手势进行即时用户反馈，这对语音输入的语音交互模型是至关重要的。在下面的用户研究中，我们将标记菜单与常规菜单进行的比较反映了几乎所有这些新的听写应用程序所使用的校正方法。

##相关工作

已经开发了各种各样的输入方法来加速触摸屏设备上的文本输入。其中一些方法类似于语音识别，因为它们利用噪声信道框架来解码原始输入信号。除了手写识别和预测复杂脚本语言（如中文）的明显例子外，软键盘还可以根据对接触点的解码动态调整键的目标区域[10]。语音识别所使用的语言模型在几乎所有的预测文本输入方法中都出现了，从T9 [9]到SWYPE [30]等形状书写技术。

除了类似于语音识别的触摸屏输入方法之外，一些研究人员已经探索如何从单词格获得更准确的识别假设，以便它们可以实时呈现。 Fink等人[7]发现提供更多正确的上下文（即更多的声学信息）可以提高准确性。同样，鲍曼等人。 [4]表明，增加语言模型中的单词重量可以提高准确性。 Selfridge等人。 [25]进一步提出了这两种观点，并提出了一种算法，用于查找格子中的路径，这些路径可以在句末（如语言模型所认为的那样）中结束，也可以集中到单个节点。这将假设的稳定性提高了33％，精度提高了21％。请注意，我们尚未尝试纳入任何这些发现，但考虑到我们未来工作的这部分内容。

关于获得实时识别结果的用户体验，Aist等人[1]向用户呈现了预先录制的消息和识别结果，这些结果既可以一次出现，也可以以增量方式出现。用户压倒性喜欢后者。 Skantze和Schlangen [27]进行了一项类似的研究，其中用户列举了一系列数字。同样，用户喜欢以增量方式查看数字。所有这些先前的研究证明了语音分类语音交互模型的合理性。据我们所知，我们在下一节中描述的用户研究代表了使用LVCSR解码将增量式实时转录与传统听写比较在自发语言生成任务上的第一次尝试。

语音键入基于手势的标记菜单与语音识别错误的多模式校正中的研究有关。在Martin等人[17]，初步识别结果暂时存储在缓冲区中，用户可以通过语音对话或鼠标交互式编辑。用户可以删除单个单词或整个缓冲区，重新说出话语，或从n最佳列表中选择单词。 Suhm等人[29]提出了对某些类型的修正切换到基于笔的交互。除了倡导拼写来代替重复说话外，他们还创建了一组笔式手势，例如删除单词以删除它们。最后，用于触摸屏设备的商用听写产品，例如iPhone Dragon Dictation应用[20]，也支持简单的基于触摸的编辑。迄今为止，这些产品都没有使用标记菜单。

##用户研究

为了评估语音输入与传统听写相比的校正效果和可用性，我们进行了一项受控实验，参与者参与了电子邮件撰写任务。对于电子邮件内容，向参与者提供了可以自行填写的结构。例如，“写一封电子邮件给你的朋友米歇尔推荐你喜欢的餐厅。建议一个她应该点的盘子，为什么她会喜欢它。“因为听写需要自发的语言生成，所以我们选择了这个任务来反映最终用户如何实际使用语音输入。

##实验设计

我们用两个独立变量进行了2×2的受试者间因素设计实验：语音交互模型（听写与语音输入）和错误修正方法（标记菜单与常规）。在常规错误更正中，所有标记菜单选项均可按以下方式提供给参与者。如果用户点击一个单词，界面将显示一个单词交替的最佳列表。如果他们对这个单词执行了按住操作，那么就会调用重新发言或拼写选项。为了删除单词，我们在文本区域的底部提供了“退格”和“删除”按钮。将光标置于单词之间，用户可以使用“Backspace”将单词删除，使用“Delete”将单词删除。用户也可以通过按住空白区域在光标所在的任何位置插入文本。

语音交互模型和纠错方法的表达顺序是平衡的。我们收集了定量和定性措施。关于量化措施，我们测量了纠正率和纠正类型。关于定性测量，我们利用美国航天局任务负荷指数（NASA-TLX）[11]，因为它被广泛用于估计感知工作量评估。它分为六个不同的问题：精神需求，实际需求，时间需求，绩效，努力和挫折。对于我们的实验，我们使用了NASA-TLX的软件版本，它包含20个分区，每个分区对应5个任务负载点。反应是在连续的100点量表上测量的。我们还通过实验后调查问卷收集了定性判断，要求参与者根据偏好对四个实验条件（听写标记菜单，语音打字标记菜单，听写规则和语音输入规则）中的每一个进行排序。等级顺序问题与NASA-TLX类似，因此我们可以准确获得工作量评估的所有维度。最后，我们收集了开放式评论，以更好地理解参与者的偏好判断。

##软硬件

我们使用Windows 7 LVCSR听写引擎开发了语音输入和听写语音交互模型。如前所述，对于语音输入，我们通过Microsoft System.Speech托管API修改了用于结束分段的沉默参数。为了控制四种实验条件下的语音准确度，我们关闭了（默认）MLLR声学适应。这两种类型的纠错方法都是使用Windows 7 Touch API和Windows Presentation Foundation（WPF）实现的。我们在配备双核2.67 GHz i7处理器和4 GB内存的HP EliteBook 2740p多点触控平板电脑上进行了实验。

##参与者

我们招募了24名参与者（12名男性和12名女性），他们都是以英语为母语的人。参与者来自各种职业背景（如金融，汽车修理工，学生，家庭主妇等）。没有参与者经常通过语音识别来使用听写。参与者的年龄从20岁到50岁不等（M = 35.13），每十年参与者人数大致相同。

##程序

总的来说，每个实验阶段历时2小时，其中包括训练LVCSR识别器，每个实验条件组成两个练习和三个实验邮件，并填写NASA-TLX和实验后问卷。为了训练LVCSR识别器，在每次会话开始时，参与者加入了Windows 7语音识别训练向导，该向导在20个句子上执行MLLR声学适应[8]，约10分钟的发言时间。我们这样做是因为我们发现，如果没有训练，识别结果是如此不准确，以至于无论语音交互模型还是错误校正方法，用户都感到沮丧。

在四个实验条件中的每一个的训练阶段期间，实验者使用两个实践电子邮件走过了交互和错误纠正风格。在第一封练习邮件中，实验者演示了不同的语音交互模型如何工作，然后执行适当的纠错方法（即重新说话，拼写，替换，删除，插入等）的各种编辑选项。使用这些选项，如果参与者即使在重试三次后仍无法纠正错误，他们被要求将其标记为不正确。一旦参与者对用户界面感到满意，他们就会在实验者的监督下自行组织第二封电子邮件。此后，培训阶段结束，用户再次撰写了3封电子邮件。在每个实验条件结束时，参与者填写NASA-TLX问卷。在实验结束时，他们填写了排名问卷并撰写了开放式评论。

##结果

###量

为了比较语音打字和听写的准确性，我们计算了一个叫做用户纠正错误率（UCER）的度量，它是在语音错误率（WER）之后建模的，这是在语音研究领域广泛使用的度量。在WER中，使用Levenshtein距离[16]将识别的单词序列与实际说出的单词序列进行比较，该距离计算了字符串编辑操作的最小数目 - 替换（S），插入（I）和删除（D） - 必要转换一个字符串到另一个。此后，WER被计算为：WER =（S + I + D）/ N，其中N是真实说出的单词序列中的单词的总数。

在我们的情况下，测量WER是不可能的，原因有两个。首先，我们没有单词序列的真实记录 - 也就是说，我们不知道用户实际上想要编写什么。其次，用户经常在看到输出并调整他们的话语形式之后即兴创作，大概是因为被识别的文本仍然捕获了他们的意图。此外，我们认为，虽然WER准确地捕捉了识别器犯下的错误百分比，但它并没有告诉我们很多关于用户为纠正识别输出而花费的努力量，至少在文本可以接受的地方。我们认为后者是接受任何听写用户界面的重要指标。因此，我们将UCER计算为：

