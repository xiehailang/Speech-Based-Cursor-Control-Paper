#针对点击式任务的非语言语音控制光标的设计和评估

**简介    **对于移动手指，手或手臂有困难的用户来说，声控光标是传统鼠标的替代物。在本文中，我们报告了一种适用于点击式任务的语音控制光标，它使用基于网格的概念访问计算机显示器上的位置，并使用非口头声音，这两种声音是用两种不同的嗡嗡声音高值和短小的牙槽摩擦音，选择所需的网格单元并执行鼠标点击事件。实验表明，与传统的鼠标网格相比，本文提出的方法可以实现更高的准确率，更短的响应时间和更好的用户偏好，从而将口头数字识别为其主要的细胞选择机制。
**关键词    **非语音，语音，网格，光标控制

##1.引言

现代操作系统的主流图形用户界面（GUI）通过视觉元素（例如图形图标和小部件）与用户交互。几乎所有的系统都支持诸如鼠标这样的指示设备作为最突出的设备，它将用户的二维输入运动转换为指向计算机屏幕上位置的多用途光标的运动，以及触发一些输入事件，例如鼠标点击和鼠标释放。尽管这些设备具有多种形式，但大多数设备都是为不具备特殊需求的普通用户而设计的。由于手指，手和手臂运动的限制或禁止，运动障碍的用户在使用这种设备的计算机上操作时有明显的困难。

基于专用于电机损坏用途的专门制造的硬件的用法来开发许多辅助装置来克服此限制。这种类型的解决方案的概念是，用户通过其他可用方式（例如不同的身体部位）将输入运动与相关事件一起发出。 [4]和[7]报道了使用眼球运动和头部运动的光标控制方案。光标控制通过用户呼吸系统产生的动作发出输入，这也是具有高实用性的另一种选择，因为典型地，这样的方法不需要通常昂贵的专门设计的硬件。在这种情况下，计算机麦克风，尺寸紧凑，重量轻，相对便宜并且可以在任何计算机系统上常见的设备就足够了。在[1]和[10]中报道了使用空气膨化的输入方案。许多基于呼吸器的光标控制方案利用自动语音识别技术将语音输入（通常是说出的单词）转换为与光标移动和鼠标事件相关的预设过程。以这种方式使用这些技术既不是语音控制游标的唯一也不是最佳选择。请注意，在本文中，我们将参考用于控制计算机显示器中的光标移动以及其相关输入事件的输入方案，通过麦克风发出可听见的声音，而不涉及除主要发音器用户的声道和喉部作为“语音光标”（VC）。

以下部分回顾了各种语音光标，并提供了一些可能导致本文提出和尝试的方法的动机的讨论。

##2.文献综述

移动光标可以被看作是两种不同的情况。第一种情况是用户想要访问显示器上的特定位置的位置访问场景，例如在启动点击事件之前将光标移动到按钮。另一种是连续移动的场景，用户希望将光标移动到连续的轨道中，例如绘制自由形式的路径时。虽然我们对这项工作中的位置访问案例感兴趣，但在这里，我们讨论以前在这两种情况下报道的与VC相关的作品，以便捕捉声音如何发声的方面，然后转化为每个提议VC中的光标移动。

如上所述，许多VC依靠自动语音识别技术，其中识别器被训练并集成到VC中，以便它们执行单词识别任务。在这些系统中，词识别器用于基于用户发出的输入信号的声波形来假设其词典或命令清单中最可能的词或项目。 Manaris et.al. [8]提出了一个基于语音的输入，其中用户的语音话语被用来识别光标目的地，并被解释为一些键盘动作。虽然单词识别方法似乎更适合于位置访问场景，但它们可以通过适当的命令集来轻松支持连续移动，例如在所说的方向移动游标的命令。

然而，已知识别精度取决于在识别器的语法中登记的词汇数量。因此，提供一套彻底覆盖所有可能行为的命令在技术上是具有挑战性的。命令集的设计需要仔细考虑语音识别器的复杂性与命令共同提供的动作范围之间的平衡。换句话说，为了保持相同的覆盖范围，如果集合中的每个命令都与更具体的动作相关联，则语音识别器对于更大的字典将更复杂。

Dai等人提出了基于网格的解决方案。 [2]减少位置访问任务的词汇量。这个鼠标网格将计算机屏幕分成九个网格，三个等距列和三个等距行。每个单元格都标有数字1到9.用户说出希望的光标目标所在单元格的标签。然后将一个新的类似划分的网格放入选定的单元格中，并重复这些步骤，直到单元格的中心位于最终目的地。 MS Windows Vista及更高版本等操作系统目前采用这种鼠标网格概念作为VC的内置解决方案。

使用单词识别与用户进行交互，在系统的响应时间方面表现出一些弱点。典型地，在大量词汇表的情况下，基于训练语音话语建立声学模型的复杂统计技术被用于处理单词识别任务，而具有较少数量词汇表的许多系统采用了类似的技术，这是由于实施方便自标准工具以及相关软件组件以来，这些系统已广泛应用。即使在解码（识别）阶段，这些统计技术也需要大量的计算资源。因此，响应时间受到影响。 Karimullah et.al. [6]也提到了他们工作中的这种延误。此外，由于单词识别任务的性质，识别必须在用户完成命令后进行。

在文献[5]中也提出了使用非语言声音或副语言，而不是VC中的基于单词的命令。为了利用非语言声音，以基于帧的方式从语音话语中提取感兴趣的声学测量，其中短时语音帧在系统接收到时分开处理。这意味着风险投资公司可以在发出声音时对这些声音做出反应。 Sporka et.al也支持此声明。 [9]谁在实时游戏场景中分析非语言控制。设计控制基于非口头声音的VC的本质是选择光标行为的声学测量。基频，能量，共振器可用作广泛使用的声学测量的一些示例，不限于VC。

以前提出的非语言VC包括声乐游戏杆[1]和U3I [10]。人声操纵杆检测语音帧的频谱属性，并将它们分为八个元音之一，每个元音分配八个方向之一。声能也被检测并转换成光标在相关元音声音方向上的移动速度。 U3I检测音高变化，并根据某些预定义音高变化模式在四个方向中的任意一个方向上移动光标。尽管在[3]中报道说，在用于访问屏幕上随机位置和用户满意率的时间方面，Vocal Joystick胜过U3I，但在[2]中进行了实验并得出结论，两者都不适合位置访问任务作为鼠标网格。

在这项工作中，我们针对一个VC进行“点击式”任务，该任务主要由位置访问任务和发出鼠标点击事件组成。我们采用非声乐声音作为我们提出的VC的声音输入，同时使用基于网格的方法来迭代地访问期望的目的地。

##3.方法
###3.1设计考虑
由于其实用性以及它被广泛采用的事实，我们的VC采用了基于网格的访问位置的思路。然而，为了选择想要的单元格而不是用户说出一个单词，我们将这个想法应用于非语言的声音。在非语言声音的情况下需要考虑的主要问题是声音的类型以及相关的声学测量值，这些声学测量值将被转换为网格中所需单元格的选择。所选非语言声音以及其相关声学测量应具有以下优选特性。

####3.1.1自然而轻松
制作声音必须是自然的，并且不应该在制作过程中与语言中使用的声音相比需要额外的努力。参与生产的咬合器运动越少，用户在长时间与VC一起使用时会感觉到的疲劳越少。

####3.1.2敏捷
所选择的声音应该是敏捷的，从某种意义上说，改变与所选择的声学测量直接相关的声音的某些属性可以迅速且自然地实现。

####3.1.3提供充分的判别能力
声音必须促进声学测量的存在，声音测量的数值分布表现出足够的变化来区分与VC相关的各种可能的选择或命令。

####3.1.4复杂度低
用于提取相关声学测量的算法必须具有较低的复杂度，以便实现感知上的实时响应。

###3.2提议的设计
嗡嗡声是由口唇闭合的准周期性振动产生的声音，被选择为用于选择网格中的细胞的非语言声音的选择。嗡嗡声的基本频率或音调从相关的嗡嗡声信号的短帧中提取，并且使用预定义的算法将音调转换为选择的网格单元的选择。为了使选择更稳健，只使用4个单元的网格而不是典型的9单元鼠标网格。网格由两个等距行和两个等距列分隔开。用户可调节的频率阈值被设置为使得其值低于阈值的音高被认为是低频音调，而超过阈值的音高是高频的。为了使这两种类型的音调能够在四种选择中进行选择，部署了顺序访问机制。图1显示了4格网格的图示。具有高频音调的嗡嗡声选择顶行中的任一个单元，而具有低频音调的单元从底行选择。根据瞬时音高值，行的选择会实时变化。在保持相同类型音调的嗡嗡声的同时，选择会在td秒之间交替地在左侧和右侧之间切换。一旦嗡嗡声终止，新的网格将占用选定的单元格，并等待基于新网格的下一次单元选择迭代。
![](/assets/2010pic1.png)
图1. 4格网格及其单元格选择。

与鼠标点击事件相关的非语言声音被选择为短暂的牙槽摩擦音，即单词“sip”中的音节初始辅音，因为其许多声学特性可以容易地与嗡嗡声分离。摩擦音是非周期性的类似噪音的声音。语言中使用的不同摩擦音在不同频率范围出现的能量数量上有不同的声学差异。如果是肺泡性的，能量集中在高频区域。该范围通常是与哼唱音高不重叠的跨度频率。

###3.3非语言声音处理算法
图2显示了声音测量是如何从声音中提取的。
![](/assets/2010pic2.png)
图2.声学测量提取。

输入声音帧的快速傅立叶变换（FFT）由两个带通滤波器BPF1和BPF2处理。 BPF1可以滤除50Hz到1kHz范围之外的任何频率分量，而BPF2滤除3.5至4.5 kHz之外的分量。滤波信号的能量标记为ELO和EHI。与谱处理并行地，计算相同语音帧的自相关，并且使用自相关峰之间的距离来计算基频，Fo或音调。

如果ELO大于嗡嗡声能量的预定阈值，则ELO大于EHI，并且Fo落在允许的频率范围内，则语音帧被检测为嗡嗡声帧。音高值被相应地使用。如果ELO和EHI通过摩擦能量的阈值，并且EHI大于ELO，则语音帧被检测为牙槽擦音帧。否则，语音框架被认为与我们的风险投资无关。

##4.实验设置
我们招募了6名参与者，其中四名男性（M1-M4）和两名女性（F1-F2），年龄从18岁到26岁不等，没有任何人有任何身体损伤。只有一名参与者（M4）拥有使用语音以及非语言鼠标网格的经验，而其他人则是新手。所有参与者都被要求使用我们的系统，一台配备2.8GHz Intel Pentium IV处理器的PC，显示屏分辨率为1280x800。

所有参与者被要求试验和比较两种情况;非语言和语音鼠标网格。非口头版本是用Matlab R2008b实现的。鼠标网格的识别器使用HTK实现，使用实时识别模式，其中使用39维MFCC向量作为语音表示，并使用5个从左至右的模型作为HMM拓扑。识别器被训练为每个参与者的依赖于说话者的识别器。用于这两种情况的接口都是用C＃编写的。

对于非语言的鼠标网格，参与者被指示在长低频和高频音调中哼唱，同时操作员调整音高，音量阈值和时间间隔td，直到达到80％的准确度。对于发言鼠标网格，参与者被要求说数字0到9，并命令“点击”在泰国和重复，直到他们达到80％的准确性。对于每个参与者，具有最佳识别率的四位数字被用作语音鼠标网格中四个单元格的标签。只有通过这项测试的人才可以参加实际的实验。在我们的案例中，一个（F2）的嗡嗡声测试失败，另一个（F1）由于系统故障无法完成非语言测试。

在实验期间，参与者旨在以最短的时间达到目​​标。有三个目标尺寸; 1）25 x 25表示小图标，2）80 x 25表示文本框，按钮等，3）60 x 60表示大图标。每一个都随机出现在屏幕上，并以相同的顺序重复四次，这占到了120个目标。
为了达到目标，参与者在非语言的鼠标网格中哼唱了短暂或长时间的低频或高频音调，同时他们说出了标记单元格的四位数字中的一个来选择语音版本中的单元格。当所有120个目标都达到或实验时间超过1小时时，实验结束。如果系统做出错误的识别，参与者可以重置当前的试用版。记录实验过程中的所有操作，以便我们可以计算相关的精度和持续时间。所有参与者也被要求填写问卷以征求他们的意见。结果在下一节中报告。

##5.结果
我们将准确率％Acc定义为从电网中选择所需电池的成功尝试的百分比。参与者重置试用的尝试被认为是不成功的。表1显示了准确率的比较。平均而言，非语言情况下的准确率较高。我们还发现没有参与者使用牙槽擦音作为点击命令有问题。

为了评估完成选择单元的单次迭代所花费的平均时间，我们根据参与者的每个记录动作分析了时间戳。在这里，我们定义一个预处理时间作为持续时间，参与者在非言语的情况下思考正确的话语，长短，高低频音调，以及在发言的情况下相应的数字鼠标格。这可以从发言和下一个发言之间的时间暗示。在非语言情况下，我们定义了一个识别时间，因为从参与者开始说话直到选择所需单元的持续时间开始，而在讲话的情况下，持续时间从讲话结束时开始到识别器返回识别结果。请注意，在语音识别器的情况下，durselect还必须包括话语本身的持续时间。表2列出了durselect的平均时间。

从结果中，我们可以观察到，每个参与者所需的平均预处理时间在两种方法之间没有显着差异。关于用于分析声音的持续时间，它们在语音识别情况下比在非语言情况下（至多1/10秒）长得多（1-2秒）。

问卷调查结果表明，使用非语言的声音更具反应性，更准确，更少累人，更有趣，但两者同样容易熟悉，而且参与者认为他们不愿意使用任何一种VC方案。

##6.结论
通过鼠标网格使用精心挑选的非语言声音通过用户的语音访问屏幕上的项目可以提供更大的用户体验，而不是依赖更出名的词语识别技术，因为它可以更快更强地处理。 在这项工作中，哼唱和牙槽摩擦音表现为非语言的声音，可以很好地与鼠标网格结合，并显示出令人满意的结果。

##参考文献
[1] Bilmes, J. A., Malkin, X. Li, J., Kilanski, K., Wright, R., Kirchhoff , K., Subramanya , A., Harada, S., Landay, J. A., Dowden , P., and Chizeck, H. The Vocal Joystick: A voice- based human-computer interface for individuals with motor impairments. In Human Language Technology Conf./Conf. on Empirical Methods in Natural Language Processing (HLT’05),( Vancouver, British Columbia, Canada).

[2] Dai, L., Goldman, R., Sears, A., and Lozier, J. Speech-based cursor control: a study of grid-based solutions. SIGACCESS Access. Comput., 77-78(2004), 94–101.

[3] Harada, S., Landay, J.A., Malkin, J., Li, X. and Bilmes, J.A. The vocal joystick: evaluation of voice-based cursor control techniques. In Proceedings of international ACM conference on Assistive technologies (ASSETS’06), (Portland, Oregon).

[4] Hornof, A. J. and Cavender, A. EyeDraw: enabling children with severe motor impairments to draw with their eyes. In Proceedings of the SIGCHI conference on Human factors in computing systems (CHI’05), (Portland, Oregon, USA)

[5] Igarashi, T., Hughes, J. F. Voice as sound: using non-verbal voice input for interactive control. In Proceedings of the annual ACM symposium on User interface software and technology (UIST’01) (Orlando, Florida, USA).

[6] Karimullah, A. S. and Sears, A. Speech-Based Cursor Control, In Proceedings of international ACM conference on Assistive technologies (ASSETS’02), (Edinburgh, Scotland).

[7] Loewenich F. and Maire F. Hands-free mouse-pointer manipulation using motion-tracking and speech recognition. In Proceedings of the 2007 conference of the computer- human interaction special interest group (CHISIG) of Australia on Computer-human interaction (OZCHI’07), (Adelaide, Australia).

[8] Manaris, B., McCauley, R. and MacGybers, V. An Intelligent Interface for Keyboard and Mouse Control – Providing Full Access to PC Functionality via Speech, In Proceedings of International Florida AI Research Symposium (FLAIRS’01), 2001, 182-188.

[9] Sporka, A. J., Kurniawan, S. H., Mahmud, M. and Slavík, P. Non-speech Input and Speech Recognition for Real-time Control of Computer Games. In Proceedings of the international ACM SIGACCESS conference on Computers and accessibility (ASSETS’06) (Portland, Oregon, USA, October 22–25, 2006).

[10] Sporka, A. J., Kurniawan, S. H., and Slavik, P. Whistling User Interface (U3I). User-Centered Interaction Paradigms for Universal Access in the Information Society, 3196 (2004), 472-478.

[11] Sporka A. J., Kurniawan S. H., Mahmud M. and Slavik P. Longitudinal study of continuous non-speech operated mouse pointer. In Proc. INTERACT 2007, Berlin, Springer, 2007, 489-502.